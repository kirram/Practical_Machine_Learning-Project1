---
title: "Practical_Machine_Learning"
author: "Kirram"
date: "October 22, 2015"
output: html_document
---


loading library:
```{r}
library(caret)
```

loading training and testing data:
```{r}
  trainingData<-read.csv("pml-training.csv")
  testingData<-read.csv("pml-testing.csv")
```

Data Processing:

1. Data Partition

```{r}
set.seed(112)
inTrain<-createDataPartition(y=trainingData$classe,p=0.7,list=FALSE)
trainingDataTRAIN<-trainingData[inTrain,]
trainingDataTEST<-trainingData[-inTrain,]
```


2. Removing values that have variance of nearly zero as they only create "noise"

```{r}
  cl1_training<-nearZeroVar(trainingDataTRAIN)
  trainingDataTRAIN<-trainingDataTRAIN[,-cl1_training]
  trainingDataTEST<-trainingDataTEST[,-cl1_training]
```

3. Removing values that frequently appear as NAs:

```{r}
  cl2<-sapply(trainingDataTRAIN,function(x) mean(is.na(x)))>0.95
  trainingDataTRAIN<-trainingDataTRAIN[,cl2==FALSE]
  trainingDataTEST<-trainingDataTEST[,cl2==FALSE]
```

4. Removing first 5 columns as they represent variables that don't add value for prediction:
```{r}
trainingDataTRAIN<-trainingDataTRAIN[,-(1:5)]
  trainingDataTEST<-trainingDataTEST[,-(1:5)]
```

Subsetting data for model testing. Before we develop and apply the predictive alghoritm to the testing data, we'll have to evaluate the results of the model to see if any further processing is required:




TRAINING:

We'll try to use Random Forests as the most advanced method described in the class. Let's use 3-fold Cross-Validation for better results:

```{r}
tc<-trainControl(method = "cv", number=3, verboseIter = FALSE)
fitM<-train(classe~.,data=trainingDataTRAIN, method="rf", trControl=tc)
```
  
Final model analysis:

```{r}
fitM$finalModel
```

The summary indicates that the model uses 500 trees with 27 variables

Now let's use our model to predict the first set of test data:

```{r}
predictions<-predict(fitM, newdata=trainingDataTEST)
```


```{r}
confusionMatrix(trainingDataTEST$classe,predictions)
```

The accuracy looks good at 99.78% with the out-of-sample error = 0.22%. This is suffitient enough to keep the model without having to try other options.

Now, let's train the model on the full training set:

Data processing

```{r}

#reloading the training data""
trainingData<-read.csv("pml-training.csv")

#Data cleaning
cl1<-nearZeroVar(trainingData)
trainingData<-trainingData[,-cl1]
testingData<-testingData[,-cl1]


cl4<-sapply(trainingData,function(x) mean(is.na(x)))>0.95
trainingData<-trainingData[,cl4==FALSE]
testingData<-testingData[,cl4==FALSE]

trainingData<-trainingData[,-(1:5)]
testingData<-testingData[,-(1:5)]
```

Training model:

```{r}
tc<-trainControl(method = "cv", number=3, verboseIter = FALSE)
fitM<-train(classe~.,data=trainingData, method="rf", trControl=tc)
```

Summary:

```{r}
fitM$finalModel
```
Results look even better now!




Final predictions on the 20 test cases:

```{r}
predictions1<-predict(fitM, newdata=testingData)
predictions1<-as.character(predictions1)

#In order to create files with predictions:

files_out<-function(x){
  
  n<-length(x)
  for(i in 1:n){
    filename<-paste0("ID_", i,".csv")
    write.table(x[i], file=filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
    
  }
  
}

# create files:

files_out(predictions1)

